## # RNA-Seq Standard from FASTQ
##
## This WDL workflow runs the STAR RNA-Seq alignment workflow for St. Jude Cloud from FASTQ input.
## The workflow takes paired input FASTQ files. 
## The read pairs are then passed through STAR alignment to generate a BAM file. The BAM is run
## through several QC steps. Quantification is done using htseq-count. 
##
## ## LICENSING
##
## #### MIT License
##
## Copyright 2020-Present St. Jude Children's Research Hospital
##
## Permission is hereby granted, free of charge, to any person obtaining a copy of this
## software and associated documentation files (the "Software"), to deal in the Software
## without restriction, including without limitation the rights to use, copy, modify, merge,
## publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons
## to whom the Software is furnished to do so, subject to the following conditions:
##
## The above copyright notice and this permission notice shall be included in all copies or
## substantial portions of the Software.
##
## THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING
## BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
## NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
## DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
## OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

version 1.1

import "../../tools/fq.wdl"
import "./rnaseq-core.wdl" as rnaseq_core_wf
import "./rnaseq-standard.wdl" as rnaseq_standard

workflow rnaseq_standard_fastq {
    parameter_meta {
        gtf: "Gzipped GTF feature file"
        star_db: "Database of reference files for the STAR aligner. The name of the root directory which was archived must match the archive's filename without the `.tar.gz` extension. Can be generated by `star-db-build.wdl`"
        read_one_fastqs: "Input Fastq format file(s) with 1st read in pair to align"
        read_two_fastqs: "Input Fastq format file(s) with 2nd read in pair to align"
        read_groups: "A space-delimited read group record for each read group. Exactly one FASTQ filename must match each read group ID from `read_one_fastqs` and `read_two_fastqs`. Read group fields (Required fields: ID, LB, PL, PU, & SM.) should be space delimited. Read groups should be comma separated, with a space on each side (e.g. ' , '). The ID field must come first for each read group and must match the basename of a FASTQ file (up to the first period). Expected form: `ID:rg1 PU:flowcell1.lane1 SM:sample1 PL:illumina LB:sample1_lib1 , ID:rg2 PU:flowcell1.lane2 SM:sample1 PL:illumina LB:sample1_lib1`"
        prefix: "Prefix for output files"
        contaminant_db: "A compressed reference database corresponding to the aligner chosen with `xenocp_aligner` for the contaminant genome"
        max_retries: "Number of times to retry failed steps. Overrides task level defaults."
        xenocp_aligner: {
            description: "Aligner to use to map reads to the host genome for detecting contamination"
            choices: [
                'bwa aln',
                'bwa mem',
                'star'
            ]
        },
        strandedness: {
            description: "Strandedness protocol of the RNA-Seq experiment. If unspecified, strandedness will be inferred by `ngsderive`."
            choices: [
                '',
                'Stranded-Reverse',
                'Stranded-Forward',
                'Unstranded'
            ]
        },
        mark_duplicates: "Add SAM flag to computationally determined duplicate reads?"
        cleanse_xenograft: "Use XenoCP to unmap reads from contaminant genome?"
        validate_input: "Ensure input FASTQs are well-formed before beginning harmonization?"
        use_all_cores: "Use all cores for multi-core steps?"
        subsample_n_reads: "Only process a random sampling of `n` reads. Any `n`<=`0` for processing entire input."
    }

    input {
        File gtf
        File star_db
        Array[File] read_one_fastqs
        Array[File] read_two_fastqs
        Array[ReadGroup] read_groups
        String prefix
        File? contaminant_db
        Int? max_retries
        String xenocp_aligner = "star"
        String strandedness = ""
        Boolean mark_duplicates = false
        Boolean cleanse_xenograft = false
        Boolean validate_input = true
        Boolean use_all_cores = false
        Int subsample_n_reads = -1
    }

    call rnaseq_standard.parse_input { input:
        input_strand=strandedness,
        cleanse_xenograft=cleanse_xenograft,
        contaminant_db=defined(contaminant_db)
    }

    scatter (rg in read_groups) {
        call ReadGroup_to_string { input: read_group=rg, max_retries=max_retries }
    }
    String stringified_read_groups = sep(
        ' , ', ReadGroup_to_string.stringified_read_group
    )

    if (validate_input){
        scatter (reads in zip(read_one_fastqs, read_two_fastqs)) {
            call fq.fqlint { input:
                read_one_fastq=reads.left,
                read_two_fastq=reads.right,
                max_retries=max_retries
            }
        }
    }

    if (subsample_n_reads > 0) {
        Int reads_per_pair = ceil(subsample_n_reads / length(read_one_fastqs))
        scatter (reads in zip(read_one_fastqs, read_two_fastqs)) {
            call fq.subsample { input:
                read_one_fastq=reads.left,
                read_two_fastq=reads.right,
                record_count=reads_per_pair,
                max_retries=max_retries
            }
        }
    }
    Array[File] selected_read_one_fastqs = select_first([
        subsample.subsampled_read1,
        read_one_fastqs
    ])
    Array[File] selected_read_two_fastqs = select_all(
        select_first([
            subsample.subsampled_read2,
            read_two_fastqs
        ])
    )

    call rnaseq_core_wf.rnaseq_core { input:
        read_one_fastqs=selected_read_one_fastqs,
        read_two_fastqs=selected_read_two_fastqs,
        read_groups=stringified_read_groups,
        prefix=prefix,
        gtf=gtf,
        star_db=star_db,
        mark_duplicates=mark_duplicates,
        contaminant_db=contaminant_db,
        cleanse_xenograft=cleanse_xenograft,
        xenocp_aligner=xenocp_aligner,
        strandedness=strandedness,
        use_all_cores=use_all_cores,
        max_retries=max_retries
    }

    output {
        File bam = rnaseq_core.bam
        File bam_index = rnaseq_core.bam_index
        File bam_checksum = rnaseq_core.bam_checksum
        File star_log = rnaseq_core.star_log
        File feature_counts = rnaseq_core.feature_counts
        File inferred_strandedness = rnaseq_core.inferred_strandedness
        String inferred_strandedness_string = rnaseq_core.inferred_strandedness_string
        File bigwig = rnaseq_core.bigwig
    }
}

struct ReadGroup {
    String ID
    String? BC
    String? CN
    String? DS
    String? DT
    String? FO
    String? KS
    String? LB
    String? PG
    String? PI
    String? PL
    String? PM
    String? PU
    String? SM
}

task ReadGroup_to_string {
    input {
        ReadGroup read_group
        Int memory_gb = 4
        Int disk_size_gb = 10
        Int max_retries = 1
    }

    command <<<
        {
            echo -n "~{'ID:~{read_group.ID} '}"
            echo -n "~{'BC:~{read_group.BC} '}"
            echo -n "~{'CN:~{read_group.CN} '}"
            echo -n "~{'DS:~{read_group.DS} '}"
            echo -n "~{'DT:~{read_group.DT} '}"
            echo -n "~{'FO:~{read_group.FO} '}"
            echo -n "~{'KS:~{read_group.KS} '}"
            echo -n "~{'LB:~{read_group.LB} '}"
            echo -n "~{'PG:~{read_group.PG} '}"
            echo -n "~{'PI:~{read_group.PI} '}"
            echo -n "~{'PL:~{read_group.PL} '}"
            echo -n "~{'PM:~{read_group.PM} '}"
            echo -n "~{'PU:~{read_group.PU} '}"
            echo "~{'SM:~{read_group.SM} '}"
        } > out.txt
    >>>

    output {
        String stringified_read_group = read_string("out.txt")
    }

    runtime {
        memory: "~{memory_gb} GB"
        disk: "~{disk_size_gb} GB"
        docker: 'ghcr.io/stjudecloud/util:1.3.0'
        maxRetries: max_retries
    }
}
